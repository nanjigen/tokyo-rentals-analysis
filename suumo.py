# Tools for scraping SUUMO
# 
import pytest
import requests, re
from time import time, sleep
from random import randint
from bs4 import BeautifulSoup
import pandas as pd
import matplotlib.pyplot as plt
from sqlalchemy import create_engine

plt.style.use('fivethirtyeight')
color_pal = plt.rcParams["axes.prop_cycle"].by_key()["color"]
# import database

# this is the URL generated after choosing specific search criteria on the website (e.g. location, house type, price range)
search_url = "http://suumo.jp/jj/chintai/ichiran/FR301FC001/?ar=030&bs=040&ta=13&sc=13101&sc=13102&sc=13103&sc=13104&sc=13105&sc=13113&cb=0.0&ct=9999999&et=9999999&cn=9999999&mb=0&mt=9999999&shkr1=03&shkr2=03&shkr3=03&shkr4=03&fw2="

def suumo_results_pages():
    """Return the number of pages generated by the search url"""
    r = requests.get(search_url)
    c = r.content
    soup = BeautifulSoup(c,"html.parser")
    all = soup.find_all("div",{"class":"cassetteitem"})
    page_nr = soup.find_all("ol",{"class":"pagination-parts"})[-1].text
    page_nr = [int(s) for s in page_nr.split() if s.isdigit()]
    page_nr = page_nr[len(page_nr)-1]
    return page_nr

def house_collector(start, end):
    """Build list of property by looping through pages of search"""
    property_list = []
    for page in range(start, end):
        r = requests.get(search_url + '&page=' + str(page))
        c = r.content
        soup = BeautifulSoup(c,"html.parser")
        house_elements = soup.find_all(lambda tag: tag.name == 'div' and
                                    tag.get('class') == ['cassetteitem'])
        # "cassetteitem" is the class for each house
        sleep(randint(1,3))
        property_list.extend(house_elements)
    return property_list

def extract_detail_text(html):
   """Extract header data from outside table"""
   house_data = []
   for item in html:
       d = {}
       d["Title"] = item.find("div",{"class","cassetteitem_content-title"}).text
       d["Locality"] = item.find("li",{"class","cassetteitem_detail-col1"}).text
       house_data.append(d)
   return house_data

def extract_table_text(html):
    """Extract text form row data in table"""
    house_data = []
    for cassetteitem in html:
        table = cassetteitem.find('table', class_='cassetteitem_other')
        rows = table.select('tbody > tr.js-cassette_link')
        for row in rows:
            columns = row.find_all('td')
            row_data = {
                'Title': cassetteitem.find('div',{'class','cassetteitem_content-title'}).text,
                'Locality': cassetteitem.find('li',{'class','cassetteitem_detail-col1'}).text,
                'Floor': columns[2].get_text().strip(),
                'Rent': columns[3].find('span', class_='cassetteitem_price--rent').text,
                'Admin Fee': columns[3].find('span', class_='cassetteitem_price--administration').get_text().strip(),
                'Deposit': columns[4].find('span', class_='cassetteitem_price--deposit').get_text().strip(),
                'Key money': columns[4].find('span', class_='cassetteitem_price--gratuity').get_text().strip(),
                'Layout': columns[5].find('span', class_='cassetteitem_madori').get_text().strip(),
                'Size': columns[5].find('span', class_='cassetteitem_menseki').get_text().strip(),
                'Link': "https://suumo.jp" + row.find('a', class_='js-cassette_link_href')['href']
                }
            house_data.append(row_data)
    return house_data

def load_data(start, end):
    df = pd.DataFrame(extract_table_text(house_collector(start, end)))
    df = df[['Title', 'Locality', 'Floor', 'Size', 'Layout', 'Rent', 'Link']]
    return df


def clean_numeric_data(dataframe):
    """Clean dataframe generated by scraping"""
    df = dataframe
    decimal_value = r'(\d+(?:\.\d+)?)'
    int_value = r'\d+'
    # Check if respective column needs cleaning
    if not len(df) == 0:
        if df['Floor'].str.contains("階").any():
            df['Floor'] = df['Floor'].apply(lambda x: re.findall(int_value, x)[0]
                                            if re.findall(int_value, x)
                                            else '')
            df['Rooms'] = df['Layout'].apply(lambda x: re.findall(int_value, x)[0]
                                        if re.findall(int_value, x)
                                        else '1' if 'ワンルーム' in x
                                        else '')
        if df['Size'].str.contains("m2").any():
            df['Size'] = df['Size'].apply(lambda x: re.findall(decimal_value, x)[0]
                                        if re.findall(decimal_value, x)
                                        else '')
        if df['Rent'].str.contains("円").any():
            # df['Rent'] = df['Rent'].apply(lambda x:
            #                             int(float(re.findall(decimal_value, x)[0]) * 1000)
            #                             if '万' in x and re.findall(decimal_value, x)
            #                             else '')
            df['Rent'] = df['Rent'].str.extract(decimal_value, expand=False)
            df['Rent'] = df['Rent'].astype(float).astype(int) * 10000
        return df


def create_database(db, table, start, end):
    engine = create_engine('sqlite:///%s' %db, echo=True)
    sqlite_table = table
    sqlite_connection = engine.connect()
    (load_data(start, end)
    .pipe(clean_numeric_data).to_sql(
        sqlite_table,
        sqlite_connection,
        if_exists='replace',
        index=False
    ))
    sqlite_connection.close()
