#+BRAIN_PARENTS: data-science
#+PROPERTY: header-args :session *tokyo-rent* :kernel python3 :mkdirp yes :noweb yes

#+TITLE: Tokyo Rentals

#+FILETAGS: incremental

* What is this?
:PROPERTIES:
:CREATED:  [2023-05-07 Sun 20:13]
:ID:       e8ce6b0d-89f0-48b4-aa28-612a1dc6cd9f
:END:

The purpose of this project is to address a problem often faced when living in Tokyo. Rent prices can vary wildly based on many factors. It also aims to demonstrate my proficiency in applying data analysis with Python.

The goal of the Tokyo Rentals project is to analyze rental prices in Tokyo and create a data-driven visualization to provide insights into the rental market in the central districts of Tokyo.

1. Data collection: Collect rental price data in Tokyo from various sources, including real estate websites, government websites, and rental agencies.
2. Data cleaning: Clean and organize the collected data by removing errors or inconsistencies and formatting it for analysis.
3. Data analysis: Analyze the data using statistical methods such as regression analysis, clustering, and time series analysis to identify patterns and trends.
4. Data visualization: Visualize the analyzed data using charts, graphs, and maps to facilitate understanding and interpretation.
5. Data sharing: Share the analyzed data and visualizations through a website or social media platforms.

** Features
:PROPERTIES:
:CREATED:  [2023-05-07 Sun 20:13]
:ID:       122187db-a3ef-4b07-b06c-6c9741dd7ab1
:END:

This project will result in an interactive Dash visualization of rental data for central districts of Tokyo. The data will be stored on Google Cloud Platform (GCP) services, and the visualization will include machine learning capabilities, as described in steimel2019.

1. Create a Dash visualization of rental data for central districts of Tokyo. The data should be stored on GCP services (such as BigQuery) and visualizations should be interactive.
2. Include demographic information as well
3. Continuous updating to build rents over time (to determine the best time of year to start renting)
4. As a stretch goal, I especially would like to include machine learning as described in steimel2019.

https://tokyocheapo.com/living/tokyo-rent-map/
https://www.datamaplab.com/posts/map-of-rent-prices-in-tokyo/
https://www.homes.co.jp/
https://console.cloud.google.com/welcome?project=tokyo-rents
https://qiita.com/tomyu/items/a08d3180b7cbe63667c9
https://github.com/georgeburry/tokyo-rental-prices
https://github.com/steimel64/Masters_Thesis_Tokyo_Rent_Prediction

** 5 Identifying next actions
:PROPERTIES:
:CREATED:  [2023-05-07 Sun 20:13]
:ID:       9c3e3b50-6197-4dfe-9c86-a8977812a2e1
:END:
Finally, you allocate the needed resources to get the project moving. It is about deciding the next actions for each of the moving parts of the project.

1. Ask questions and define the problem.
2. Prepare data by collecting and storing the information.
3. Process data by cleaning and checking the information.
4. Analyze data to find patterns, relationships, and trends.
5. Share data with your audience.
6. Act on the data and use the analysis results.
*** TODO Incorporate external data sources
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       7c683a07-c5b7-4fab-9949-ebd965ad8e41
:END:
Explore the incorporation of external data sources, such as transportation accessibility or neighborhood characteristics, to enhance the analysis.

*** TODO Experiment with machine learning algorithms
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       6a119376-ed7b-4cf4-a5c5-01e7b25271df
:END:
Experiment with machine learning algorithms, as described in "steimel2019," to predict rental prices or identify influential factors.

*** TODO Collaborate with UX/UI designers
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       06239226-00b9-4e1b-a9b8-040654137474
:END:
Collaborate with UX/UI designers to enhance the user experience of the rental data visualization dashboard.

*** TODO Evaluate model performance and accuracy
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       b602aa69-a988-4f68-8657-d725d276ee92
:END:
Evaluate the performance and accuracy of rental price predictions or analyses and iterate on models or methodologies.

*** TODO Document data analysis process
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       a3cfd92b-41b4-4cea-94f4-63e7b4176cbb
:END:
Document the data analysis process, including data sources, cleaning steps, analysis techniques, and visualization choices.

*** TODO Gather user feedback
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       79411ece-5426-4630-ba3b-758e69a75c2e
:END:
Gather user feedback on the rental data visualization and incorporate suggestions for further improvements.

*** TODO Explore presentation opportunities
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       c844f103-d627-4ca6-a6c7-645bc753c032
:END:
Explore opportunities to present findings and insights at data analytics or real estate-related conferences or meetups in Tokyo.

*** TODO Update and maintain the project
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       68cbf7f4-207e-40b5-b897-80be1d041959
:END:
Continuously update and maintain the rental data analysis project to provide the latest rental information for users.
* NEXT Data Collection: Import rental data
:PROPERTIES:
:CREATED:  [2023-05-13 Sat 09:30]
:ID:       f0f14775-e4a4-4644-9825-cad597f29c00
:END:

https://tuto-techno-guix-hpc.gitlabpages.inria.fr/guidelines/
https://lists.gnu.org/archive/html/guix-devel/2019-10/msg00511.html
https://github.com/jkitchin/ox-ipynb
https://github.com/sj50179/Google-Data-Analytics-Professional-Certificate/wiki/1.1.3.Understanding-the-data-ecosystem
https://gitlab.inria.fr/guix-hpc/guix-kernel

** Requirements
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 12:05]
:ID:       3d73e3bd-690b-47d1-af42-d18a8c973bf5
:END:
First well define out requirements.
First those we can define by ~Guix~'s own packages:
#+begin_src scheme :tangle manifest.scm :eval no
(specifications->manifest
  (list "python"
        "python-ipython"
        "python-ipykernel"
        "jupyter"
        "emacs-jupyter"
        "guix-jupyter"
        "python-beautifulsoup4"
        "python-pandas"
        "sqlite"
        "python-sqlalchemy"))
#+end_src

# TODO add section for activing env
#+begin_src elisp
(progn
  (require 'jupyter)
  (find-file "/docker:ecstatic_knuth:/home/nandev/test.py")
  (org-babel-jupyter-aliases-from-kernelspecs t)
  (org-reload))
#+end_src

And then imported into Python:
#+begin_src jupyter-python :noweb-ref requirements :results silent
import requests, re
from time import time, sleep
from random import randint
from bs4 import BeautifulSoup
import pandas as pd
import matplotlib.pyplot as plt
from sqlalchemy import create_engine

plt.style.use('fivethirtyeight')
color_pal = plt.rcParams["axes.prop_cycle"].by_key()["color"]
# import database
#+end_src

** Scraping from SUUMO
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 15:03]
:ID:       0fb79f3f-eb9b-4ee6-9910-ca58f356604c
:END:

Previous projects have used [[https://suumo.jp/][SUUMO]], a popular rental search platform. Typical of Japanese websites, there is no API, and instead web-scraping must be utilized.

A common approach seems to be to generate a reusable URL seeded with specific search criteria via its [[https://suumo.jp/jj/chintai][chintai]] search page (which will likely reroute based on region).

At first glance this seems brittle, but due to the aforementioned quirk of Japan's web services, there is some durability to links as sites rarely change or at least not in breaking ways.

Take for instance the following link, which was used in a [[https://github.com/georgeburry/tokyo-rental-prices/tree/master][similar project]] in 2018:
#+begin_src jupyter-python :noweb-ref search-url :eval yes :results silent
# this is the URL generated after choosing specific search criteria on the website (e.g. location, house type, price range)
search_url = "http://suumo.jp/jj/chintai/ichiran/FR301FC001/?ar=030&bs=040&ta=13&sc=13101&sc=13102&sc=13103&sc=13104&sc=13105&sc=13113&cb=0.0&ct=9999999&et=9999999&cn=9999999&mb=0&mt=9999999&shkr1=03&shkr2=03&shkr3=03&shkr4=03&fw2="
#+end_src

*** Parsing
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 08:37]
:ID:       551be45d-5803-4e1b-ae3c-8afd7a4e172e
:END:

Lets do an initial parsing of the web content by counting the number of pages returned via a function. By inspecting the HTML elements we need to look inside the class =cassetteitem=. All entries related to the search are split into pages by looking for =pagination-parts= class instances.

#+begin_src jupyter-python :noweb-ref scrape-functions :eval yes :results silent
def suumo_results_pages():
    """Return the number of pages generated by the search url"""
    r = requests.get(search_url)
    c = r.content
    soup = BeautifulSoup(c,"html.parser")
    all = soup.find_all("div",{"class":"cassetteitem"})
    page_nr = soup.find_all("ol",{"class":"pagination-parts"})[-1].text
    page_nr = [int(s) for s in page_nr.split() if s.isdigit()]
    page_nr = page_nr[len(page_nr)-1]
    return page_nr

#+end_src

#+begin_src jupyter-python :eval yes
print(suumo_results_pages(),"pages were found")
#+end_src

#+RESULTS:
: 700 pages were found

As we can see, the original still works - albeit with more results than the original.

*** Collection of house elements
:PROPERTIES:
:CREATED:  [2023-05-28 Sun 12:59]
:ID:       63efe878-e4dd-4ce8-875e-112b46c34442
:END:

Iterate pages by adding page number to end of search URL each loop.
Build this into the target collection of houses by filling the =property_list= variable with the house_elements list via the ~.extend~ method:

#+begin_src jupyter-python :noweb-ref scrape-functions :results silent
def house_collector(start_page, end_page):
    """Build list of property by looping through pages of search"""
    # property_list = []
    session = requests.Session()
    paginated_url = search_url + '&page='

    for page in range(start, end):
        try:
            response = session.get(paginated_url + str(page))
            response.raise_for_status()  # Raise an exception if the request was not successful
            # c = r.content
            soup = BeautifulSoup(response.content,"html.parser")
            # house_elements = soup.find_all(lambda tag: tag.name == 'div' and
                                        # tag.get('class') == ['cassetteitem'])
            # "cassetteitem" is the class for each house
            yield from soup.select('div.cassetteitem')
            sleep(randint(1,3))
        except requests.exceptions.RequestException as e:
            print(f"Error occurred while fetching page {page}: {e}")
            # property_list.extend(house_elements)
    property_list = list(generate_house_elements)
    return property_list

#+end_src

We can tell that a given page contains 30 results:
#+begin_src jupyter-python :eval yes
# print(house_collector(1, 2)[1])
len(house_collector(1, 2))
#+end_src

#+RESULTS:
: 30

Lets test for this to make sure we're getting the same kind of results for a given page:
#+begin_src jupyter-python :noweb-ref tests :results silent
def test_house_collector():
    expected_houses = 30
    houses = house_collector(1, 2) ;; TODO use randint() = x-1
    # Check if the houses are collected correctly
    assert len(houses) == 30
    # for house, expected_house in zip(houses, expected_houses):
    #     assert house.text.strip() == expected_house
#+end_src


*** Title details
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 13:40]
:ID:       f8b43fd2-5b08-4b4f-affe-ab5873da3515
:END:

The initial header of a given entry is contained in the =cassetteitem-detail= div, and contains the building name and some other information note found in the table used later on. For each house discovered, let's collect information on title, locality, and put the information into a dictionary:

#+begin_src jupyter-python :noweb-ref scrape-functions :results silent
def extract_detail_text(html):
   """Extract header data from outside table"""
   house_data = []
   for item in html:
       d = {}
       d["Title"] = item.find("div",{"class","cassetteitem_content-title"}).text
       d["Locality"] = item.find("li",{"class","cassetteitem_detail-col1"}).text
       house_data.append(d)
   return house_data

#+end_src

As we can see, this gives us what we're looking for.
#+begin_src jupyter-python
print(extract_detail_text(house_collector(1, 2))[0])
#+end_src

#+RESULTS:
: {'Title': 'ボストーク・ネオ', 'Locality': '東京都千代田区東神田２'}

*** Table extraction
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 13:41]
:ID:       7ed8c278-8155-4a58-9c73-027683515ad1
:END:

'間取り' (madori) refers to the house plan, rendered in the =XLDK= format, where X is the number of rooms and D and K respectively refer to Dining room and Kitchen, and are optional. As is standard with Japanese listings, this is also often accompanied by an actual floor plan graphic.

TODO, use title function in place of explicit entry below.
#+begin_src jupyter-python :noweb-ref scrape-functions :results silent
def extract_table_text(html):
    """Extract text form row data in table"""
    house_data = []
    for cassetteitem in html:
        table = cassetteitem.find('table',{'class','cassetteitem_other'})
        rows = table.find_all('tr', class_='js-cassette_link')
        for row in rows:
            columns = row.find_all('td')
            row_data = {
                'Title': cassetteitem.find('div',{'class','cassetteitem_content-title'}).text,
                'Locality': cassetteitem.find('li',{'class','cassetteitem_detail-col1'}).text,
                'Floor': columns[2].get_text().strip(),
                'Rent': columns[3].find('span', class_='cassetteitem_price--rent').text,
                'Admin Fee': columns[3].find('span', class_='cassetteitem_price--administration').get_text().strip(),
                'Deposit': columns[4].find('span', class_='cassetteitem_price--deposit').get_text().strip(),
                'Key money': columns[4].find('span', class_='cassetteitem_price--gratuity').get_text().strip(),
                'Layout': columns[5].find('span', class_='cassetteitem_madori').get_text().strip(),
                'Size': columns[5].find('span', class_='cassetteitem_menseki').get_text().strip(),
                'Link': "https://suumo.jp" + row.find('a', class_='js-cassette_link_href')['href']
                }
            house_data.append(row_data)
    return house_data

#+end_src

Getting the first member of the generated list shows a desirable dictionary entry:
#+begin_src jupyter-python
print(extract_table_text(house_collector(1, 2))[1])
#+end_src

#+RESULTS:
: {'Title': 'トルナーレ日本橋浜町', 'Locality': '東京都中央区日本橋浜町３', 'Floor': '36階', 'Rent': '19万円', 'Admin Fee': '10000円', 'Deposit': '19万円', 'Key money': '19万円', 'Layout': 'ワンルーム', 'Size': '44.01m2', 'Link': 'https://suumo.jp/chintai/jnc_000082906762/?bc=100325224283'}
*** Load dataframe function
:PROPERTIES:
:CREATED:  [2023-06-04 Sun 09:14]
:ID:       ed9d39e5-119a-42a2-a619-a7ae5ea63a32
:END:

Let's create a simple function to load the df into memory for the given range.
#+begin_src jupyter-python :noweb-ref scrape-functions :results silent
def load_data(start, end):
    df = pd.DataFrame(extract_table_text(house_collector(start, end)))
    df = df[['Title', 'Locality', 'Floor', 'Size', 'Layout', 'Rent', 'Link']]
    return df

#+end_src

Lets take a look at the initial frame:
#+begin_src jupyter-python :results yes
df = load_data(1, 2)
df.head()
#+end_src

#+RESULTS:
#+begin_example
              Title   Locality Floor     Size Layout    Rent  \
0  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    5階  65.72m2   3LDK    33万円
1  ザ・グランクラッセ日本橋イースト  東京都中央区新川２   12階  65.72m2   3LDK  33.7万円
2  ザ・グランクラッセ日本橋イースト  東京都中央区新川２   11階   71.7m2   3LDK  35.3万円
3  ザ・グランクラッセ日本橋イースト  東京都中央区新川２   12階   71.7m2   3LDK  35.4万円
4  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    7階  71.44m2   3LDK  35.4万円

                                                Link
0  https://suumo.jp/chintai/jnc_000079775721/?bc=...
1  https://suumo.jp/chintai/jnc_000082788184/?bc=...
2  https://suumo.jp/chintai/jnc_000080944199/?bc=...
3  https://suumo.jp/chintai/jnc_000082788185/?bc=...
4  https://suumo.jp/chintai/jnc_000082479900/?bc=...
#+end_example

As we can see, our frame is created correctly, however there are entries that are non-numeric which we actually want as number values in order to begin EDA:
#+begin_src jupyter-python
df['Rent'].dtype
#+end_src

#+RESULTS:
: dtype('O')

Which is not supported by =Numpy=.

** TODO Research and identify additional rental data sources
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       7c6311eb-30e3-4144-9b35-fe323edcf08f
:END:
Research and identify additional sources of rental data in Tokyo to enrich the dataset.

* TODO Cleaning
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       8c93d6a6-282a-4890-974d-0c209b874cf2
:END:
** NEXT Apply data cleaning techniques
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       e79c734c-70ef-4230-9911-806019735e1c
:TRIGGER:  chain-find-next(NEXT,from-current,priority-up,effort-down)
:END:
Apply data cleaning techniques to address inconsistencies, missing values, and outliers in the rental data.

We need to reconfigure our data frame so that relevant columns contain numerical values. We also will be inserting a new column =Rooms= to represent how many liveable rooms there are without losing access to the XLDK layout convention:

#+begin_src jupyter-python :noweb-ref clean-functions :results silent
def clean_numeric_data(dataframe):
    """Clean dataframe generated by scraping"""
    df = dataframe
    decimal_value = r'(\d+(?:\.\d+)?)'
    int_value = r'\d+'
    # Check if respective column needs cleaning
    if not len(df) == 0:
        if df['Floor'].str.contains("階").any():
            df['Floor'] = df['Floor'].apply(lambda x: re.findall(int_value, x)[0]
                                            if re.findall(int_value, x)
                                            else '')
            df['Rooms'] = df['Layout'].apply(lambda x: re.findall(int_value, x)[0]
                                        if re.findall(int_value, x)
                                        else '1' if 'ワンルーム' in x
                                        else '')
        if df['Size'].str.contains("m2").any():
            df['Size'] = df['Size'].apply(lambda x: re.findall(decimal_value, x)[0]
                                        if re.findall(decimal_value, x)
                                        else '')
        if df['Rent'].str.contains("円").any():
            # df['Rent'] = df['Rent'].apply(lambda x:
            #                             int(float(re.findall(decimal_value, x)[0]) * 1000)
            #                             if '万' in x and re.findall(decimal_value, x)
            #                             else '')
            df['Rent'] = df['Rent'].str.extract(decimal_value, expand=False)
            df['Rent'] = df['Rent'].astype(float).astype(int) * 10000
        return df

#+end_src

Now lets apply our data cleaning and take a look at the new frame:
#+begin_src jupyter-python
df_cleaned = clean_numeric_data(load_data(1, 2))
df_cleaned.head()
#+end_src

#+RESULTS:
#+begin_example
              Title   Locality Floor   Size Layout    Rent  \
0  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     5  65.72   3LDK  330000
1  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12  65.72   3LDK  330000
2  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    11   71.7   3LDK  350000
3  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12   71.7   3LDK  350000
4  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     7  71.44   3LDK  350000

                                                Link Rooms
0  https://suumo.jp/chintai/jnc_000079775721/?bc=...     3
1  https://suumo.jp/chintai/jnc_000082788184/?bc=...     3
2  https://suumo.jp/chintai/jnc_000080944199/?bc=...     3
3  https://suumo.jp/chintai/jnc_000082788185/?bc=...     3
4  https://suumo.jp/chintai/jnc_000082479900/?bc=...     3
#+end_example

Our Rent column returns as the correct datatype:
#+begin_src jupyter-python
df_cleaned['Rent']
#+end_src

#+RESULTS:
#+begin_example
0      330000
1      330000
2      350000
3      350000
4      350000
        ...
209    150000
210    150000
211    150000
212    150000
213    160000
Name: Rent, Length: 214, dtype: int64
#+end_example

#+begin_src jupyter-python
df_cleaned.loc[1]
# df[df['Title'] == 'クリオ日本橋久松町']
# df.loc[1, 'Link']
#+end_src

#+RESULTS:
: Title                                        ザ・グランクラッセ日本橋イースト
: Locality                                            東京都中央区新川２
: Floor                                                      12
: Size                                                    65.72
: Layout                                                   3LDK
: Rent                                                   330000
: Link        https://suumo.jp/chintai/jnc_000082788184/?bc=...
: Rooms                                                       3
: Name: 1, dtype: object

* TODO Develop data collection pipeline
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       630ccbf5-6f99-40ae-9f6e-2ec5541f04c2
:END:
Develop a data collection pipeline or script to automate the gathering of rental data from various sources.

We'll to use the ~pandas.DataFrame.pipe~ to setup a simple data pipeline that runs from extraction via scraping, through our exploratory and cleaning dataframe transformations and ending in loading into an sqlite3 database; effectively giving us an ETL pipeline.

This demonstrated below:
#+begin_src jupyter-python
df_cleaned = load_data(1, 2).pipe(clean_numeric_data)
df_cleaned.head()
#+end_src

#+RESULTS:
#+begin_example
              Title   Locality Floor   Size Layout    Rent  \
0  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     5  65.72   3LDK  330000
1  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12  65.72   3LDK  330000
2  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    11   71.7   3LDK  350000
3  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12   71.7   3LDK  350000
4  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     7  71.44   3LDK  350000

                                                Link Rooms
0  https://suumo.jp/chintai/jnc_000079775721/?bc=...     3
1  https://suumo.jp/chintai/jnc_000082788184/?bc=...     3
2  https://suumo.jp/chintai/jnc_000080944199/?bc=...     3
3  https://suumo.jp/chintai/jnc_000082788185/?bc=...     3
4  https://suumo.jp/chintai/jnc_000082479900/?bc=...     3
#+end_example

Now lets have this as reproducible functions.

*** Piped database creation
:PROPERTIES:
:CREATED:  [2023-06-04 Sun 16:20]
:ID:       01b405a2-05c5-4280-8b78-fbab4d28e433
:END:

Here we set variable name with the string of a yearly table. Then we use that variable when invoking the =to_sql= method on the piped object, which returns a cleaned pandas DataFrame.

We set the =if_exists= arguments to ='replace'= so that the code doesn't fail if the table already exists in the database. We can also change =if_exists= to ='append'= and add exception handling in a more robust version of this program.

Create =suumo.db= and establish connection to resultant database:
#+begin_src jupyter-python :noweb-ref database-functions :results slient
def create_database(db, table, start, end):
    engine = create_engine('sqlite:///%s' %db, echo=True)
    sqlite_table = table
    sqlite_connection = engine.connect()
    (load_data(start, end)
    .pipe(clean_numeric_data).to_sql(
        sqlite_table,
        sqlite_connection,
        if_exists='replace',
        index=False
    ))
    sqlite_connection.close()

#+end_src

Turn into test
#+begin_src jupyter-python :results silent :eval no
create_database("suumo-test.db", "Suumo2023_test", 1, 2)
#+end_src

#+end_example

* TODO Analysis
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       8ce6c8e1-1d6e-4321-a723-b3e1e4892cb3
:END:
** TODO Perform exploratory data analysis
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       32c93679-55fa-4e6a-9ce0-5e2125d0213d
:END:
Perform exploratory data analysis to gain insights into rental price distribution, property types, and geographical variations.

** TODO Implement statistical analysis techniques
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       00a104bd-41e3-4f87-ae4e-c6741fa4ef09
:END:
Implement statistical analysis techniques such as regression, clustering, or time series analysis to identify patterns and trends in the rental market.

* TODO Visualization
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       0bfc3db3-552e-458f-8127-5761d40b4eb2
:END:
*** TODO Create interactive visualizations
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       947558a4-7652-4a83-89e4-8e69b031f364
:END:
Create interactive visualizations using Dash or other libraries to present rental data in an intuitive and user-friendly manner.
*** TODO Conduct comparative analysis
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       0cd53336-9b42-49a2-873a-566cc58678fd
:END:
Conduct comparative analysis between different districts or neighborhoods within Tokyo to identify affordable rental options or investment opportunities.

* TODO Data Sharing
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       0d22c9cc-a8e8-45fa-927d-7369eceae898
:END:

* Files
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 18:06]
:ID:       9007fc1c-9c66-434a-8cb3-5227d6b0d9c0
:END:

** suumo.py
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 18:07]
:ID:       4396c626-15b4-4752-ad41-3ead8942475e
:END:

#+begin_src python :tangle suumo.py :eval no
# Tools for scraping SUUMO
<<requirements>>

<<search-url>>

<<scrape-functions>>

<<clean-functions>>

#+end_src
