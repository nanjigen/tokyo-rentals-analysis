#+BRAIN_PARENTS: data-science
#+PROPERTY: header-args :session *tokyo-rent* :kernel python3 :mkdirp yes :noweb yes

#+TITLE: Tokyo Rentals

#+FILETAGS: incremental

* What is this?
:PROPERTIES:
:CREATED:  [2023-05-07 Sun 20:13]
:ID:       e8ce6b0d-89f0-48b4-aa28-612a1dc6cd9f
:END:

The purpose of this project is to address a problem often faced when living in Tokyo. Rent prices can vary wildly based on many factors.

These include but are not limited to:
- Square meterage
- Property age
- Location
- Number of rooms
- Distance from nearest station

So how do you know if you are making a good choice? How might we weigh different factors against each other in order to determine if a given property is worth its monthly rent?

To find out we'll need to collect and analyze rental prices in Tokyo and create a data-driven visualization to provide insights into the rental market in the central districts of its famous 23 wards. We will attempt to follow best-practices in both data analysis and data engineering:

1. Data collection: Collect rental price data in Tokyo from various sources, including real estate websites, government websites, and rental agencies.
2. Data cleaning: Clean and organize the collected data by removing errors or inconsistencies and formatting it for analysis.
3. Data analysis: Analyze the data using statistical methods such as regression analysis, clustering, and time series analysis to identify patterns and trends.
4. Data visualization: Visualize the analyzed data using charts, graphs, and maps to facilitate understanding and interpretation.
5. Data sharing: Share the analyzed data and visualizations through a website or social media platforms.

Some questions to answer:

- "What are we trying to solve for?"
- "What is the ideal rental location for a given set of criteria?"
- "What tools will be used?"
- "How can we make the research as reproduce-able as possible?"
- "How can I demonstrate my proficiency with Python, data analytics and Google cloud?"

** Features
:PROPERTIES:
:CREATED:  [2023-05-07 Sun 20:13]
:ID:       122187db-a3ef-4b07-b06c-6c9741dd7ab1
:END:

This project will result in an interactive Dash visualization of rental data for central districts of Tokyo. The data will be stored on Google Cloud Platform (GCP) services, and the visualization will include machine learning capabilities, as described in steimel2019.

1. Create a Dash visualization of rental data for central districts of Tokyo. The data should be stored on GCP services (such as BigQuery) and visualizations should be interactive.
2. Include demographic information as well
3. Continuous updating to build rents over time (to determine the best time of year to start renting)
4. As a stretch goal, I especially would like to include machine learning as described in steimel2019.

https://tokyocheapo.com/living/tokyo-rent-map/
https://www.datamaplab.com/posts/map-of-rent-prices-in-tokyo/
https://www.homes.co.jp/
https://console.cloud.google.com/welcome?project=tokyo-rents
https://qiita.com/tomyu/items/a08d3180b7cbe63667c9
https://github.com/georgeburry/tokyo-rental-prices
https://github.com/steimel64/Masters_Thesis_Tokyo_Rent_Prediction

** 5 Identifying next actions
:PROPERTIES:
:CREATED:  [2023-05-07 Sun 20:13]
:ID:       9c3e3b50-6197-4dfe-9c86-a8977812a2e1
:END:
Finally, you allocate the needed resources to get the project moving. It is about deciding the next actions for each of the moving parts of the project.

1. Ask questions and define the problem.
2. Prepare data by collecting and storing the information.
3. Process data by cleaning and checking the information.
4. Analyze data to find patterns, relationships, and trends.
5. Share data with your audience.
6. Act on the data and use the analysis results.
*** TODO Incorporate external data sources
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       7c683a07-c5b7-4fab-9949-ebd965ad8e41
:END:
Explore the incorporation of external data sources, such as transportation accessibility or neighborhood characteristics, to enhance the analysis.

*** TODO Experiment with machine learning algorithms
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       6a119376-ed7b-4cf4-a5c5-01e7b25271df
:END:
Experiment with machine learning algorithms, as described in "steimel2019," to predict rental prices or identify influential factors.

*** TODO Collaborate with UX/UI designers
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       06239226-00b9-4e1b-a9b8-040654137474
:END:
Collaborate with UX/UI designers to enhance the user experience of the rental data visualization dashboard.

*** TODO Evaluate model performance and accuracy
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       b602aa69-a988-4f68-8657-d725d276ee92
:END:
Evaluate the performance and accuracy of rental price predictions or analyses and iterate on models or methodologies.

*** TODO Document data analysis process
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       a3cfd92b-41b4-4cea-94f4-63e7b4176cbb
:END:
Document the data analysis process, including data sources, cleaning steps, analysis techniques, and visualization choices.

*** TODO Gather user feedback
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       79411ece-5426-4630-ba3b-758e69a75c2e
:END:
Gather user feedback on the rental data visualization and incorporate suggestions for further improvements.

*** TODO Explore presentation opportunities
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       c844f103-d627-4ca6-a6c7-645bc753c032
:END:
Explore opportunities to present findings and insights at data analytics or real estate-related conferences or meetups in Tokyo.

*** TODO Update and maintain the project
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       68cbf7f4-207e-40b5-b897-80be1d041959
:END:
Continuously update and maintain the rental data analysis project to provide the latest rental information for users.

* Project Setup
:PROPERTIES:
:TRIGGER:  chain-find-next(NEXT,from-current,priority-up,effort-down)
:CREATED:  [2023-08-17 Thu 13:53]
:ID:       5f0e9a12-9560-414c-8b34-ccdc1f4156df
:END:

https://gitlab.inria.fr/guix-hpc/guix-kernel
https://tuto-techno-guix-hpc.gitlabpages.inria.fr/guidelines/

Here we make sure our environment and packages are correctly setup. Note the use of Guix and Jupyter.

** Startup
:PROPERTIES:
:CREATED:  [2023-08-01 Tue 15:52]
:ID:       260dd424-89d2-4172-9a4c-5be905661ccc
:END:

This is some magic to get ~conda~ working in a docker container. The official package from the ~guix~ repos isn't currently compiling so we have ~tramp~ communicate to the docker instance with ~docker-tramp~. This is run via buffer local variables at the end of the org file.

TODO Test (tramp connection?) so that it only needs to be run once per emacs session.
#+name: startup
#+begin_src elisp :tangle settings.el :results silent
(progn
  (load "ob-jupyter")
  (if (featurep 'docker-core)
      (find-file "/docker:ecstatic_knuth:/home/nandev/test.py"))
  ;;     (find-file-noselect "/docker:ecstatic_knuth:/home/nandev/test.py")) ;; FIXME I don't think this works to trigger the docker package
  (org-babel-jupyter-aliases-from-kernelspecs t)
  (org-reload))
#+end_src

** Requirements
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 12:05]
:ID:       3d73e3bd-690b-47d1-af42-d18a8c973bf5
:END:

Specify requirements.

First those we can define by ~Guix~'s own packages:
#+begin_src scheme :tangle manifest.scm :eval no
(specifications->manifest
  (list "python"
        "python-ipython"
        "python-ipykernel"
        "python-pytest"
        "jupyter"
        "emacs-jupyter"
        "guix-jupyter"
        "python-beautifulsoup4"
        "python-pandas"
        "python-seaborn"
        "bash"
        "font-google-noto"
        "sqlite"
        "python-sqlalchemy"))
#+end_src

# TODO run conversion to requirements.txt file

** Imports
:PROPERTIES:
:CREATED:  [2023-08-01 Tue 15:52]
:ID:       182f0a90-3d08-4ae1-afa8-4249c8df89a4
:END:

And then imported into Python:
#+begin_src python :noweb-ref imports :results silent
import requests, re
import json
# import pytest
from time import time, sleep
from random import randint
from bs4 import BeautifulSoup
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
fprop = fm.FontProperties(fname='/fonts/NotoSansCJKjp-Regular.otf')

import seaborn as sns
# sns.set(font='NotoSansCJKjp-Regular.otf')
sns.set(font='Noto Sans CJK JP')

from sqlalchemy import create_engine

plt.style.use('fivethirtyeight')
color_pal = plt.rcParams["axes.prop_cycle"].by_key()["color"]
# import database

#+end_src

Get Japanese fonts to display in matplotlib and seaborn:
https://medium.com/@rocsky/how-to-let-matplotlib-support-chinese-without-install-font-ccde385d088a
#+begin_src python :noweb-ref imports :results silent
# [f for f in fm.fontManager.ttflist if 'Noto' in f.name]
# print(fm.matplotlib_fname())
# matplotlib.font_manager.findSystemFonts()
from matplotlib import pyplot as plt,font_manager as fm
from pathlib import Path
import os
#Restore the `.rcParams` from Matplotlib's internal default style.
plt.rcdefaults()

path = Path(os.getcwd())
# fname=os.path.join(path.parent.absolute(),'data','NotoSansCJKjp-Regular.otf')
fname=os.path.join(path.absolute(),'fonts','NotoSansCJKjp-Regular.otf')
fontProperties=fm.FontProperties(fname=fname,size=14)
default_font=fontProperties.get_name()# "Arial Unicode MS"
if default_font not in [f.name for f in fm.fontManager.ttflist]:
    print(f"{default_font} does not exist, let's add it to fontManager" )

if fname not in [f.fname for f in fm.fontManager.ttflist]:
    fm.fontManager.addfont(fname) # need absolute path

plt.rcParams['font.sans-serif']=[default_font]+plt.rcParams['font.sans-serif']
plt.rcParams['axes.unicode_minus']=False # in case minus sign is shown as box
# "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"
#+end_src

* NEXT Data Collection: Import rental data
:PROPERTIES:
:CREATED:  [2023-05-13 Sat 09:30]
:ID:       f0f14775-e4a4-4644-9825-cad597f29c00
:END:

In this initial phase, we will be collecting rental data. Initially from well-known rental sites in Japan, but later also government and other data which might allow us to better answer our research questions.

** Scraping from SUUMO
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 15:03]
:ID:       0fb79f3f-eb9b-4ee6-9910-ca58f356604c
:END:

Previous projects have used [[https://suumo.jp/][SUUMO]], a popular rental search platform. Typical of Japanese websites, there is no API, and instead web-scraping must be utilized.

A common approach seems to be to generate a reusable URL seeded with specific search criteria via its [[https://suumo.jp/jj/chintai][chintai]] search page (which will likely reroute based on region).

At first glance this seems brittle, but due to the aforementioned quirk of Japan's web services, there is some durability to links as sites rarely change or at least not in breaking ways.

Take for instance the following link, which was used in a [[https://github.com/georgeburry/tokyo-rental-prices/tree/master][similar project]] in 2018:
#+begin_src web :noweb-ref old-suumo-url
http://suumo.jp/jj/chintai/ichiran/FR301FC001/?ar=030&bs=040&ta=13&sc=13101&sc=13102&sc=13103&sc=13104&sc=13105&sc=13113&cb=0.0&ct=9999999&et=9999999&cn=9999999&mb=0&mt=9999999&shkr1=03&shkr2=03&shkr3=03&shkr4=03&fw2=
#+end_src

*** Parsing
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 08:37]
:ID:       551be45d-5803-4e1b-ae3c-8afd7a4e172e
:END:

Initial exploration of the =search-url= page was done via Firefox's Web Developer tools. By inspecting the HTML elements, its determined that we need to look inside the class =cassetteitem= to find property related information. All entries related to the search are split into pages, which we can see by looking for =pagination-parts= class instances.

First create a [[https://docs.python-requests.org/en/latest/user/advanced/][Session object]] outside our functions via =requests.Session()= for connection pooling to improve performance. This can be a constant:

#+begin_src python :noweb-ref constants :results silent
session = requests.Session()
#+end_src

Then target the 2018 url:
#+begin_src python :noweb-ref constants :results silent
# this is the old URL generated after choosing specific search criteria on the website (e.g. location, house type, price range)
search_url = "<<old-suumo-url>>"
#+end_src

Now we can do an initial check of the web content by retrieving the number of pages returned via a function. Here we access the text in the =pagination-parts= class which returns a string containing select-able page numbers. After splitting as integers, we access the last member of the resultant string, giving us the number corresponding the last page button:

#+begin_src python :noweb-ref scrape-functions :eval yes :results silent
def fetch_total_pages_count(search_url):
    """Return the number of pages generated by the search url"""
    try:
        response = session.get(search_url)
        soup = BeautifulSoup(response.content, "html.parser")
        page_button_text = soup.find("ol", class_="pagination-parts").text
        last_page_number = int(page_button_text.split()[-1])

        return last_page_number

    except (requests.RequestException, AttributeError, ValueError, IndexError) as e:
            print(f"Error fetching total pages count: {e}")
            return None

#+end_src

#+begin_src python :exports both :eval no
print(fetch_total_pages_count(search_url),"pages were found")
#+end_src

#+RESULTS:
: 700 pages were found

As we can see, the original link still works - albeit with more results than the original. This can be seen visually on the webpage and is accessible in the =pagination.pagination_set-nav= class.

[[file:img/pagination_set.png]]

We can also see a large number of hits, over 300, 000. We can characterize the number of search results given by SUUMO with the following, which targets the =pagination_set-hit= =div=, splitting the formatted html string at the '件' counter:

#+begin_src python :noweb-ref scrape-functions :eval yes :results silent
def fetch_results_total_hits(search_url):
    """Return the number of search result hits"""
    response = session.get(search_url)
    soup = BeautifulSoup(response.content, "html.parser")
    div_element = div_element = soup.find('div', class_='pagination_set-hit')
    results_hits = int(''.join(div_element.strings).split('件')[0].strip())
    return results_hits

#+end_src

#+begin_src python :exports both :eval no
print(fetch_results_total_hits(search_url), "search result hits")
#+end_src

#+RESULTS:
: 304801 search result hits

Above this area on page we can see more search result display options:

[[file:img/tab-ui.png]]

If we look on the right at 表示建物数, "Number of Displayed Buildings", we can see that it is set to 30 per page:

[[file:img/tab-ui-results.png]]

We can target this number with the following code. The dropdown menu has the id =js-tabmenu2-pcChange=. The value is a nested =<option>= as a child of the =<select>=  element, the currently chosen option having the =selected= attribute:

#+begin_src python :noweb-ref scrape-functions :eval yes :results silent
def fetch_results_per_page(search_url):
    """Return the selected displayed results per page"""
    response = session.get(search_url)
    soup = BeautifulSoup(response.content, "html.parser")
    select_element = soup.find('select', id='js-tabmenu2-pcChange')
    selected_option = select_element.find('option', selected=True)
    selected_value = int(selected_option['value'])
    return selected_value

#+end_src

#+begin_src python :exports both :eval yes
print(fetch_results_per_page(search_url), "results per page")
#+end_src

#+RESULTS:
: 30 results per page

With a little math we can see that the total number of pages and the results per page don't add up to SUUMO's returned search results hits:

#+name: calculate results
#+begin_src python
fetch_results_per_page(search_url) * fetch_total_pages_count(search_url)
#+end_src

#+RESULTS:
: 21030

Clearly shy of our expected results. Let's create a unit test against this case:

#+begin_src python :noweb-ref tests :results silent
def test_search_results(search_url):
    """Test the number of returned search results against results per page * page count"""
    expected_result_count = fetch_results_total_hits(search_url)
    calculated_result_count = <<calculate results>>
    assert expected_result_count == calculated_result_count, \
        f"Expected {expected_result_count} results, but got {calculated_result_count} caculated results."

#+end_src

Running the assertion gives us an error:
#+begin_src python :results output
test_search_results(search_url)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
[0;31m---------------------------------------------------------------------------[0m
[0;31mAssertionError[0m                            Traceback (most recent call last)
Cell [0;32mIn [136], line 1[0m
[0;32m----> 1[0m [43mtest_search_results[49m[43m([49m[43msearch_url[49m[43m)[49m

Cell [0;32mIn [125], line 5[0m, in [0;36mtest_search_results[0;34m(search_url)[0m
[1;32m      3[0m expected_result_count [38;5;241m=[39m fetch_results_total_hits(search_url)
[1;32m      4[0m calculated_result_count [38;5;241m=[39m fetch_results_per_page(search_url) [38;5;241m*[39m fetch_total_pages_count(search_url)
[0;32m----> 5[0m [38;5;28;01massert[39;00m expected_result_count [38;5;241m==[39m calculated_result_count, \
[1;32m      6[0m     [38;5;124mf[39m[38;5;124m"[39m[38;5;124mExpected [39m[38;5;132;01m{[39;00mexpected_result_count[38;5;132;01m}[39;00m[38;5;124m results, but got [39m[38;5;132;01m{[39;00mcalculated_result_count[38;5;132;01m}[39;00m[38;5;124m results.[39m[38;5;124m"[39m

[0;31mAssertionError[0m: Expected 307628 results, but got 21120 results.
#+end_example
:END:
*** Collection of rental listing elements
:PROPERTIES:
:CREATED:  [2023-05-28 Sun 12:59]
:ID:       63efe878-e4dd-4ce8-875e-112b46c34442
:END:

Here we write a function who's aim is to collect the html of all rental listings in a given page range.

1. Iterate pages by suffixing a pagination keyword (=&page=) on the url, adding page number to end of search URL each loop.
2. Use a more specific CSS selector for =cassetteitem=, =div.cassetteitem=.
3. Basic rate limiting via a random sleep
4. Build this into the target collection of rental listings with a generator at =rental_listings=, saving on memory by yielding rental elements one by one.
5. ~try~ and ~except~ for raising errors on pulling a particular page.


#+begin_src python :noweb-ref scrape-functions :results silent
def collect_rental_listings(search_url, start_page, end_page):
    """Collect rental listings by looping through search result pages."""
    paginated_url = search_url + '&page='

    def fetch_listing_elements():
        for page in range(start_page, end_page):
            try:
                response = session.get(paginated_url + str(page))
                response.raise_for_status()  # Raise an exception if the request was not successful
                soup = BeautifulSoup(response.content,"html.parser")
                # "cassetteitem" is the class for each rental
                yield from soup.select('div.cassetteitem')
                sleep(randint(1,3))
            except requests.exceptions.RequestException as e:
                print(f"Error occurred while fetching page {page}: {e}")

    rental_listings = list(fetch_listing_elements())
    return rental_listings

#+end_src

We can tell that a given page contains 30 results, here we test the first page:
#+begin_src python :eval yes
sum(1 for _ in collect_rental_listings(search_url, 0, 1))
#+end_src

#+RESULTS:
: 30

Lets test for this to make sure we're getting the same kind of results for a given page:
#+begin_src python :noweb-ref tests :eval no
def test_number_of_rental_listings():
    """Test if the expected number of rental listings are collected per page."""
    expected_listings = 30
    assert sum(1 for _ in collect_rental_listings(search_url, 1, 2)) == expected_listings
#+end_src

Here we [[https://docs.pytest.org/en/7.1.x/how-to/usage.html][invoke pytest]] at the command-line to run a singular test function via its ~nodeid~ using the =::= syntax.
Note the use of the ~-q~ (quiet) and ~--disable-warnings~ flags. These ensure low verbosity output in our ~RESULTS~ drawer, and will be the de facto for all in-buffer tests in the rest of the document, each test appearing after its respective function.

Results are piped to ~tr~ for formatting.
#+begin_src sh :session tests :results code :eval yes
pytest -q --disable-warnings tests/test_suumo.py::test_number_of_rental_listings | tr -s ' '
#+end_src

#+RESULTS:
#+begin_src sh
. [100%]
1 passed, 1 warning in 10.15s
#+end_src

*** Title details
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 13:40]
:ID:       f8b43fd2-5b08-4b4f-affe-ab5873da3515
:END:

The initial header of a given entry is contained in the =cassetteitem-detail= div, and contains the building name and some other information note found in the table used later on. For each house discovered, let's collect information on title, locality, and put the information into a dictionary:

#+begin_src python :noweb-ref scrape-functions :results silent
def extract_detail_text(html):
    """Extract header data from outside table"""
    house_data = []
    for item in html:
        d = {}
        d["Title"] = item.find("div",{"class","cassetteitem_content-title"}).text
        d["Locality"] = item.find("li",{"class","cassetteitem_detail-col1"}).text
        house_data.append(d)
    return house_data

#+end_src

As we can see, this gives us what we're looking for.
#+begin_src python :eval no
print(extract_detail_text(house_collector(1, 2))[0])
#+end_src

#+RESULTS:
: {'Title': 'アジールコート芝公園', 'Locality': '東京都港区芝２'}

We won't end up using this code in the extraction phase.

*** Table extraction
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 13:41]
:ID:       7ed8c278-8155-4a58-9c73-027683515ad1
:END:

Within each =cassetteitem= ~div~, there is a table containing the individual apartments associated with that particular building. These are stored in ~tr~ rows with the class =js-cassette_link=.

#+begin_src python
def bukken_by_row(html):
    house_data = []
    for cassetteitem in html:
        try:
            title = cassetteitem.find("div", class_="cassetteitem_content-title").text.strip()
            table = cassetteitem.find('table', class_='cassetteitem_other')
            rows = table.select('tbody > tr.js-cassette_link')
            # rows = cassetteitem.select('table.cassetteitem_other tbody > tr.js-cassette_link')
            house_data = [row.find('span.cassetteitem_menseki').text.strip() for row in rows]

            yield title, house_data
            # for row in rows:
            #     area = row.find('span', class_='cassetteitem_menseki').text.strip()

            #     house_data.append(area)
            # print(title)
        except AttributeError:
            # Handle missing elements or other exceptions
            pass
    # return house_data

sum(1 for _ in bukken_by_row(house_collector(1, 2)))
# bukken_by_row(house_collector(1, 2))

#+end_src

#+RESULTS:
: 0

'間取り' (madori) refers to the house plan, rendered in the =XLDK= format, where X is the number of rooms and D and K respectively refer to Dining room and Kitchen, and are optional. As is standard with Japanese listings, this is also often accompanied by an actual floor plan graphic.

TODO, use title function in place of explicit entry below.
#+begin_src python :noweb-ref scrape-functions :results silent
def extract_house_data(html):
    """Extract text from row data in table"""
    house_data = []
    for cassetteitem in html:
        table = cassetteitem.find('table',{'class','cassetteitem_other'})
        rows = table.find_all('tr', class_='js-cassette_link')
        for row in rows:
            columns = row.find_all('td')
            row_data = {
                'Title': extract_title(cassetteitem),
                'Locality': extract_locality(cassetteitem),
                'Floor': extract_floor(columns),
                'Rent': extract_rent(columns),
                'Admin Fee': extract_admin_fee(columns),
                'Deposit': extract_deposit(columns),
                'Key money': extract_key_money(columns),
                'Layout': extract_layout(columns),
                'Size': extract_size(columns),
                'ID': extract_id(columns),
                'Coordinates': extract_gps_location(row),
                'Link': extract_link(row),
            }
            house_data.append(row_data)
    return house_data

#+end_src

#+begin_src python :noweb-ref scrape-functions :results silent
def extract_title(cassetteitem):
    return cassetteitem.find('div', {'class', 'cassetteitem_content-title'}).text

def extract_locality(cassetteitem):
    return cassetteitem.find('li', {'class', 'cassetteitem_detail-col1'}).text

def extract_floor(cassetteitem):
    columns = cassetteitem.find_all('td')
    return columns[2].get_text().strip()

def extract_rent(cassetteitem):
    columns = cassetteitem.find_all('td')
    return columns[3].find('span', class_='cassetteitem_price--rent').text

def extract_admin_fee(cassetteitem):
    columns = cassetteitem.find_all('td')
    admin_fee = columns[3].find('span', class_='cassetteitem_price--administration')
    return admin_fee.get_text().strip() if admin_fee else ''

def extract_deposit(cassetteitem):
    columns = cassetteitem.find_all('td')
    deposit = columns[4].find('span', class_='cassetteitem_price--deposit')
    return deposit.get_text().strip() if deposit else ''

def extract_key_money(cassetteitem):
    columns = cassetteitem.find_all('td')
    key_money = columns[4].find('span', class_='cassetteitem_price--gratuity')
    return key_money.get_text().strip() if key_money else ''

def extract_layout(cassetteitem):
    columns = cassetteitem.find_all('td')
    layout = columns[5].find('span', class_='cassetteitem_madori')
    return layout.get_text().strip() if layout else ''

def extract_size(cassetteitem):
    columns = cassetteitem.find_all('td')
    size = columns[5].find('span', class_='cassetteitem_menseki')
    return size.get_text().strip() if size else ''

def extract_link(cassetteitem):
    row = cassetteitem.find('tr', class_='js-cassette_link')
    link = row.find('a', class_='js-cassette_link_href')
    return "https://suumo.jp" + link['href'] if link else ''

#+end_src

Getting the first member of the generated list shows a desirable dictionary entry:
print(extract_table_text(house_collector(1, 2))[1])
#+begin_src python
#+end_src

#+RESULTS:
: {'Title': 'トルナーレ日本橋浜町', 'Locality': '東京都中央区日本橋浜町３', 'Floor': '36階', 'Rent': '19万円', 'Admin Fee': '10000円', 'Deposit': '19万円', 'Key money': '19万円', 'Layout': 'ワンルーム', 'Size': '44.01m2', 'Link': 'https://suumo.jp/chintai/jnc_000082906762/?bc=100325224283'}
*** Load dataframe function
:PROPERTIES:
:CREATED:  [2023-06-04 Sun 09:14]
:ID:       ed9d39e5-119a-42a2-a619-a7ae5ea63a32
:END:

Let's create a simple function to load the df into memory for the given results page range.
Use =try-except= block to catch exceptions during the data loading process.
# TODO Consider using types

#+begin_src python :noweb-ref scrape-functions :results silent
def load_data(start_page, end_page):
    """Load the data into a DataFrame for the given results page range."""
    try:
        extracted_data = extract_house_data(house_collector(start_page, end_page))
        df = pd.DataFrame(extracted_data, columns=['Title', 'Locality', 'Floor', 'Size', 'Layout', 'Rent', 'Link'])
        return df
    except Exception as e:
        print(f"Error occurred while loading data: {e}")
        return None

#+end_src

Lets take a look at the initial frame:
#+begin_src python :results yes
df = load_data(1, 2)
df.head()
#+end_src

#+RESULTS:
#+begin_example
              Title   Locality Floor     Size Layout    Rent  \
0  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    5階  65.72m2   3LDK    33万円
1  ザ・グランクラッセ日本橋イースト  東京都中央区新川２   12階  65.72m2   3LDK  33.7万円
2  ザ・グランクラッセ日本橋イースト  東京都中央区新川２   11階   71.7m2   3LDK  35.3万円
3  ザ・グランクラッセ日本橋イースト  東京都中央区新川２   12階   71.7m2   3LDK  35.4万円
4  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    7階  71.44m2   3LDK  35.4万円

                                                Link
0  https://suumo.jp/chintai/jnc_000079775721/?bc=...
1  https://suumo.jp/chintai/jnc_000082788184/?bc=...
2  https://suumo.jp/chintai/jnc_000080944199/?bc=...
3  https://suumo.jp/chintai/jnc_000082788185/?bc=...
4  https://suumo.jp/chintai/jnc_000082479900/?bc=...
#+end_example

As we can see, our frame is created correctly, however there are entries that are non-numeric which we actually want as number values in order to begin EDA:
#+begin_src python
df['Rent'].dtype
#+end_src

#+RESULTS:
: dtype('O')

Which is not supported by =Numpy=.

** TODO Research and identify additional rental data sources
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       7c6311eb-30e3-4144-9b35-fe323edcf08f
:END:
Research and identify additional sources of rental data in Tokyo to enrich the dataset.

* TODO Cleaning
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       8c93d6a6-282a-4890-974d-0c209b874cf2
:END:
** NEXT Apply data cleaning techniques
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       e79c734c-70ef-4230-9911-806019735e1c
:TRIGGER:  chain-find-next(NEXT,from-current,priority-up,effort-down)
:END:
Apply data cleaning techniques to address inconsistencies, missing values, and outliers in the rental data.

We need to reconfigure our data frame so that relevant columns contain numerical values. We also will be inserting a new column =Rooms= to represent how many liveable rooms there are without losing access to the XLDK layout convention:

Use input validation to ensure a valid Pandas DataFrame or Series and use a DataFrame Copy to ensure immutablity of original dataframe.

For speed I use pre-compiled regexes via =re.compile()= outside the function body. Finally we do a simple test of the OG df to see if it needs to be cleaned, and further tests of unwanted strings in the respective columns before applying the reconfigures to avoid multiplying values unnecessarily.
#+begin_src python :noweb-ref clean-functions :results silent
def clean_numeric_data(dataframe: pd.DataFrame) -> pd.DataFrame:
    """
    Clean the dataframe generated by scraping to address inconsistencies, missing values, and outliers.

    Args:
        dataframe (pd.DataFrame): The input DataFrame to be cleaned.

    Returns:
        pd.DataFrame: The cleaned DataFrame.
    """
    if not isinstance(dataframe, (pd.DataFrame, pd.Series)):
        raise ValueError("Input must be a Pandas DataFrame or Series.")

    df = dataframe.copy()

    # Pre-compile regular expressions
    decimal_value = re.compile(r'(\d+(?:\.\d+)?)')
    int_value = re.compile(r'\d+')

    # Check if respective column needs cleaning
    if not df.empty:
        if df['Floor'].str.contains("階").any():
            df['Floor'] = df['Floor'].apply(lambda x: re.findall(int_value, x)[0]
                                            if re.findall(int_value, x)
                                            else '')
            df['Rooms'] = df['Layout'].apply(lambda x: re.findall(int_value, x)[0]
                                        if re.findall(int_value, x)
                                        else '1' if 'ワンルーム' in x
                                        else '')
        if df['Size'].str.contains("m2").any():
            df['Size'] = df['Size'].apply(lambda x: re.findall(decimal_value, x)[0]
                                        if re.findall(decimal_value, x)
                                        else '')
        if df['Rent'].str.contains("円").any():
            # df['Rent'] = df['Rent'].apply(lambda x:
            #                             int(float(re.findall(decimal_value, x)[0]) * 1000)
            #                             if '万' in x and re.findall(decimal_value, x)
            #                             else '')
            df['Rent'] = df['Rent'].str.extract(decimal_value, expand=False)
            df['Rent'] = df['Rent'].astype(float).astype(int) * 10000
        return df

#+end_src

Now lets apply our data cleaning and take a look at the new frame:
#+begin_src python
df_cleaned = clean_numeric_data(load_data(1, 2))
df_cleaned.head()
#+end_src

#+RESULTS:
#+begin_example
              Title   Locality Floor   Size Layout    Rent  \
0  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     5  65.72   3LDK  330000
1  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12  65.72   3LDK  330000
2  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    11   71.7   3LDK  350000
3  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12   71.7   3LDK  350000
4  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     7  71.44   3LDK  350000

                                                Link Rooms
0  https://suumo.jp/chintai/jnc_000079775721/?bc=...     3
1  https://suumo.jp/chintai/jnc_000082788184/?bc=...     3
2  https://suumo.jp/chintai/jnc_000080944199/?bc=...     3
3  https://suumo.jp/chintai/jnc_000082788185/?bc=...     3
4  https://suumo.jp/chintai/jnc_000082479900/?bc=...     3
#+end_example

Our Rent column returns as the correct datatype:
#+begin_src python
df_cleaned['Rent']
#+end_src

#+RESULTS:
#+begin_example
0      330000
1      330000
2      350000
3      350000
4      350000
        ...
209    150000
210    150000
211    150000
212    150000
213    160000
Name: Rent, Length: 214, dtype: int64
#+end_example

#+begin_src python
df_cleaned.loc[1]
# df[df['Title'] == 'クリオ日本橋久松町']
# df.loc[1, 'Link']
#+end_src

#+RESULTS:
: Title                                        ザ・グランクラッセ日本橋イースト
: Locality                                            東京都中央区新川２
: Floor                                                      12
: Size                                                    65.72
: Layout                                                   3LDK
: Rent                                                   330000
: Link        https://suumo.jp/chintai/jnc_000082788184/?bc=...
: Rooms                                                       3
: Name: 1, dtype: object

* TODO Develop data collection pipeline
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       630ccbf5-6f99-40ae-9f6e-2ec5541f04c2
:END:
Develop a data collection pipeline or script to automate the gathering of rental data from various sources.

We'll to use the ~pandas.DataFrame.pipe~ to setup a simple data pipeline that runs from extraction via scraping, through our exploratory and cleaning dataframe transformations and ending in loading into an sqlite3 database; effectively giving us an ETL pipeline.

This demonstrated below:
df_cleaned = load_data(1, 2).pipe(clean_numeric_data)
df_cleaned.head()
#+begin_src python
#+end_src

#+RESULTS:
#+begin_example
              Title   Locality Floor   Size Layout    Rent  \
0  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     5  65.72   3LDK  330000
1  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12  65.72   3LDK  330000
2  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    11   71.7   3LDK  350000
3  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12   71.7   3LDK  350000
4  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     7  71.44   3LDK  350000

                                                Link Rooms
0  https://suumo.jp/chintai/jnc_000079775721/?bc=...     3
1  https://suumo.jp/chintai/jnc_000082788184/?bc=...     3
2  https://suumo.jp/chintai/jnc_000080944199/?bc=...     3
3  https://suumo.jp/chintai/jnc_000082788185/?bc=...     3
4  https://suumo.jp/chintai/jnc_000082479900/?bc=...     3
#+end_example

Now lets have this as reproducible functions.

*** Piped database creation
:PROPERTIES:
:CREATED:  [2023-06-04 Sun 16:20]
:ID:       01b405a2-05c5-4280-8b78-fbab4d28e433
:END:

Here we set variable name with the string of a yearly table. Then we use that variable when invoking the =to_sql= method on the piped object, which returns a cleaned pandas DataFrame.

We set the =if_exists= arguments to ='replace'= so that the code doesn't fail if the table already exists in the database. We can also change =if_exists= to ='append'= and add exception handling in a more robust version of this program.

Create =suumo.db= and establish connection to resultant database:
#+begin_src python :noweb-ref database-functions :results silent
def create_database(db, table, start_page, end_page):
    engine = create_engine('sqlite:///%s' %db, echo=True)
    sqlite_table = table
    sqlite_connection = engine.connect()
    (load_data(start, end)
    .pipe(clean_numeric_data).to_sql(
        sqlite_table,
        sqlite_connection,
        if_exists='replace',
        index=False
    ))
    sqlite_connection.close()

#+end_src

Turn into test
create_database("suumo-test.db", "Suumo2023_test", 1, 2)
#+begin_src python :results silent :eval yes
#+end_src

#+end_example

* TODO Analysis
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       8ce6c8e1-1d6e-4321-a723-b3e1e4892cb3
:END:
** TODO Perform exploratory data analysis
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       32c93679-55fa-4e6a-9ce0-5e2125d0213d
:END:
Perform exploratory data analysis to gain insights into rental price distribution, property types, and geographical variations.

#+begin_src python
** TODO Implement statistical analysis techniques
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       00a104bd-41e3-4f87-ae4e-c6741fa4ef09
:END:
Implement statistical analysis techniques such as regression, clustering, or time series analysis to identify patterns and trends in the rental market.

* TODO Visualization
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       0bfc3db3-552e-458f-8127-5761d40b4eb2
:END:
*** TODO Create interactive visualizations
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       947558a4-7652-4a83-89e4-8e69b031f364
:END:
Create interactive visualizations using Dash or other libraries to present rental data in an intuitive and user-friendly manner.
*** TODO Conduct comparative analysis
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       0cd53336-9b42-49a2-873a-566cc58678fd
:END:
Conduct comparative analysis between different districts or neighborhoods within Tokyo to identify affordable rental options or investment opportunities.

* TODO Data Sharing
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       0d22c9cc-a8e8-45fa-927d-7369eceae898
:END:

* Files
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 18:06]
:ID:       9007fc1c-9c66-434a-8cb3-5227d6b0d9c0
:header-args: :eval no :noweb yes
:END:

#+begin_src toml :tangle pyproject.toml
[tool.pytest.ini_options]
pythonpath = [
  ".", "src",
]
#+end_src

** .gitignore
:PROPERTIES:
:CREATED:  [2023-08-16 Wed 15:36]
:ID:       47aba52e-f4c1-4da5-b2e0-b8eb4cacb0d7
:END:

=.gitignore= based off of [[https://github.com/github/gitignore/blob/main/Python.gitignore][Github's great boilerplate]]:

#+begin_src conf :tangle .gitignore
 # Byte-compiled / optimized / DLL files
__pycache__/
#+end_src

** src
:PROPERTIES:
:CREATED:  [2023-06-07 Wed 18:14]
:ID:       d05dcf01-a5e4-4263-b9dd-0ec0550e3db2
:END:

#+begin_src python :tangle src/suumo/__init__.py

#+end_src

*** suumo.py
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 18:07]
:ID:       4396c626-15b4-4752-ad41-3ead8942475e
:END:

#+begin_src python :tangle src/suumo/suumo.py
# Tools for scraping SUUMO
# <<requirements>>
<<imports>>

# Constants
<<constants>>

<<scrape-functions>>

<<clean-functions>>

<<database-functions>>
#+end_src

** tests/tests.py
:PROPERTIES:
:CREATED:  [2023-06-05 Mon 20:06]
:ID:       421ebaa6-4722-4adc-a9d2-8e8667193d85
:END:

#+begin_src python :tangle tests/test_suumo.py
from suumo.suumo import *

<<tests>>

#+end_src

#+begin_src python :tangle tests/__init__.py

#+end_src

