#+BRAIN_PARENTS: data-science
#+PROPERTY: header-args :session *tokyo-rent* :kernel python3 :mkdirp yes :noweb yes

#+TITLE: Tokyo Rentals

#+FILETAGS: incremental

* What is this?
:PROPERTIES:
:CREATED:  [2023-05-07 Sun 20:13]
:ID:       e8ce6b0d-89f0-48b4-aa28-612a1dc6cd9f
:END:

The purpose of this project is to address a problem often faced when living in Tokyo. Rent prices can vary wildly based on many factors.

These include but are not limited to:
- Square meterage
- Age
- Location
- Number of rooms
- Distance from nearest station

It also aims to demonstrate my proficiency in applying data analysis with Python.

We'll analyze rental prices in Tokyo and create a data-driven visualization to provide insights into the rental market in the central districts of Tokyo.

1. Data collection: Collect rental price data in Tokyo from various sources, including real estate websites, government websites, and rental agencies.
2. Data cleaning: Clean and organize the collected data by removing errors or inconsistencies and formatting it for analysis.
3. Data analysis: Analyze the data using statistical methods such as regression analysis, clustering, and time series analysis to identify patterns and trends.
4. Data visualization: Visualize the analyzed data using charts, graphs, and maps to facilitate understanding and interpretation.
5. Data sharing: Share the analyzed data and visualizations through a website or social media platforms.

Some questions to answer:

- "What are we trying to solve for?"
- "What is the ideal rental location for a given set of criteria?"
- "What tools will be used?"
- "How can we make the research as reproduce-able as possible?"
- "How can I demonstrate my proficiency with Python, data analytics and Google cloud?"

** Features
:PROPERTIES:
:CREATED:  [2023-05-07 Sun 20:13]
:ID:       122187db-a3ef-4b07-b06c-6c9741dd7ab1
:END:

This project will result in an interactive Dash visualization of rental data for central districts of Tokyo. The data will be stored on Google Cloud Platform (GCP) services, and the visualization will include machine learning capabilities, as described in steimel2019.

1. Create a Dash visualization of rental data for central districts of Tokyo. The data should be stored on GCP services (such as BigQuery) and visualizations should be interactive.
2. Include demographic information as well
3. Continuous updating to build rents over time (to determine the best time of year to start renting)
4. As a stretch goal, I especially would like to include machine learning as described in steimel2019.

https://tokyocheapo.com/living/tokyo-rent-map/
https://www.datamaplab.com/posts/map-of-rent-prices-in-tokyo/
https://www.homes.co.jp/
https://console.cloud.google.com/welcome?project=tokyo-rents
https://qiita.com/tomyu/items/a08d3180b7cbe63667c9
https://github.com/georgeburry/tokyo-rental-prices
https://github.com/steimel64/Masters_Thesis_Tokyo_Rent_Prediction

** 5 Identifying next actions
:PROPERTIES:
:CREATED:  [2023-05-07 Sun 20:13]
:ID:       9c3e3b50-6197-4dfe-9c86-a8977812a2e1
:END:
Finally, you allocate the needed resources to get the project moving. It is about deciding the next actions for each of the moving parts of the project.

1. Ask questions and define the problem.
2. Prepare data by collecting and storing the information.
3. Process data by cleaning and checking the information.
4. Analyze data to find patterns, relationships, and trends.
5. Share data with your audience.
6. Act on the data and use the analysis results.
*** TODO Incorporate external data sources
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       7c683a07-c5b7-4fab-9949-ebd965ad8e41
:END:
Explore the incorporation of external data sources, such as transportation accessibility or neighborhood characteristics, to enhance the analysis.

*** TODO Experiment with machine learning algorithms
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       6a119376-ed7b-4cf4-a5c5-01e7b25271df
:END:
Experiment with machine learning algorithms, as described in "steimel2019," to predict rental prices or identify influential factors.

*** TODO Collaborate with UX/UI designers
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       06239226-00b9-4e1b-a9b8-040654137474
:END:
Collaborate with UX/UI designers to enhance the user experience of the rental data visualization dashboard.

*** TODO Evaluate model performance and accuracy
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       b602aa69-a988-4f68-8657-d725d276ee92
:END:
Evaluate the performance and accuracy of rental price predictions or analyses and iterate on models or methodologies.

*** TODO Document data analysis process
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       a3cfd92b-41b4-4cea-94f4-63e7b4176cbb
:END:
Document the data analysis process, including data sources, cleaning steps, analysis techniques, and visualization choices.

*** TODO Gather user feedback
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       79411ece-5426-4630-ba3b-758e69a75c2e
:END:
Gather user feedback on the rental data visualization and incorporate suggestions for further improvements.

*** TODO Explore presentation opportunities
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       c844f103-d627-4ca6-a6c7-645bc753c032
:END:
Explore opportunities to present findings and insights at data analytics or real estate-related conferences or meetups in Tokyo.

*** TODO Update and maintain the project
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       68cbf7f4-207e-40b5-b897-80be1d041959
:END:
Continuously update and maintain the rental data analysis project to provide the latest rental information for users.
* NEXT Data Collection: Import rental data
:PROPERTIES:
:CREATED:  [2023-05-13 Sat 09:30]
:ID:       f0f14775-e4a4-4644-9825-cad597f29c00
:END:

https://tuto-techno-guix-hpc.gitlabpages.inria.fr/guidelines/
https://lists.gnu.org/archive/html/guix-devel/2019-10/msg00511.html
https://github.com/jkitchin/ox-ipynb
https://github.com/sj50179/Google-Data-Analytics-Professional-Certificate/wiki/1.1.3.Understanding-the-data-ecosystem
https://gitlab.inria.fr/guix-hpc/guix-kernel

** Requirements
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 12:05]
:ID:       3d73e3bd-690b-47d1-af42-d18a8c973bf5
:END:

Specify requirements.

First those we can define by ~Guix~'s own packages:
#+begin_src scheme :tangle manifest.scm :eval no
(specifications->manifest
  (list "python"
        "python-ipython"
        "python-ipykernel"
        "python-pytest"
        "jupyter"
        "emacs-jupyter"
        "guix-jupyter"
        "python-beautifulsoup4"
        "python-pandas"
        "python-seaborn"
        "bash"
        "font-google-noto"
        "sqlite"
        "python-sqlalchemy"))
#+end_src

# TODO run conversion to requirements.txt file

** Startup
:PROPERTIES:
:CREATED:  [2023-08-01 Tue 15:52]
:ID:       260dd424-89d2-4172-9a4c-5be905661ccc
:END:

This is some magic to get ~conda~ working in a docker container. The official package from the ~guix~ repos isn't currently compiling so we have ~tramp~ communicate to the docker instance with ~docker-tramp~. This is run via buffer local variables at the end of the org file.

#+name: startup
#+begin_src elisp :tangle settings.el :results silent
(progn
  (load "ob-jupyter")
  (if (featurep 'docker-core)
      (find-file "/docker:ecstatic_knuth:/home/nandev/test.py"))
  ;;     (find-file-noselect "/docker:ecstatic_knuth:/home/nandev/test.py")) ;; FIXME I don't think this works to trigger the docker package
  (org-babel-jupyter-aliases-from-kernelspecs t)
  (org-reload))
#+end_src

** Imports
:PROPERTIES:
:CREATED:  [2023-08-01 Tue 15:52]
:ID:       182f0a90-3d08-4ae1-afa8-4249c8df89a4
:END:

And then imported into Python:
#+begin_src python :noweb-ref imports :results silent
import requests, re
import json
# import pytest
from time import time, sleep
from random import randint
from bs4 import BeautifulSoup
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
fprop = fm.FontProperties(fname='/fonts/NotoSansCJKjp-Regular.otf')

import seaborn as sns
# sns.set(font='NotoSansCJKjp-Regular.otf')
sns.set(font='Noto Sans CJK JP')

from sqlalchemy import create_engine

plt.style.use('fivethirtyeight')
color_pal = plt.rcParams["axes.prop_cycle"].by_key()["color"]
# import database

#+end_src

Get Japanese fonts to display in matplotlib and seaborn:
https://medium.com/@rocsky/how-to-let-matplotlib-support-chinese-without-install-font-ccde385d088a
#+begin_src python :noweb-ref imports :results silent
# [f for f in fm.fontManager.ttflist if 'Noto' in f.name]
# print(fm.matplotlib_fname())
# matplotlib.font_manager.findSystemFonts()
from matplotlib import pyplot as plt,font_manager as fm
from pathlib import Path
import os
#Restore the `.rcParams` from Matplotlib's internal default style.
plt.rcdefaults()

path = Path(os.getcwd())
# fname=os.path.join(path.parent.absolute(),'data','NotoSansCJKjp-Regular.otf')
fname=os.path.join(path.absolute(),'fonts','NotoSansCJKjp-Regular.otf')
fontProperties=fm.FontProperties(fname=fname,size=14)
default_font=fontProperties.get_name()# "Arial Unicode MS"
if default_font not in [f.name for f in fm.fontManager.ttflist]:
    print(f"{default_font} does not exist, let's add it to fontManager" )

if fname not in [f.fname for f in fm.fontManager.ttflist]:
    fm.fontManager.addfont(fname) # need absolute path

plt.rcParams['font.sans-serif']=[default_font]+plt.rcParams['font.sans-serif']
plt.rcParams['axes.unicode_minus']=False # in case minus sign is shown as box
# "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc"
#+end_src

** Scraping from SUUMO
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 15:03]
:ID:       0fb79f3f-eb9b-4ee6-9910-ca58f356604c
:END:

Previous projects have used [[https://suumo.jp/][SUUMO]], a popular rental search platform. Typical of Japanese websites, there is no API, and instead web-scraping must be utilized.

A common approach seems to be to generate a reusable URL seeded with specific search criteria via its [[https://suumo.jp/jj/chintai][chintai]] search page (which will likely reroute based on region).

At first glance this seems brittle, but due to the aforementioned quirk of Japan's web services, there is some durability to links as sites rarely change or at least not in breaking ways.

Take for instance the following link, which was used in a [[https://github.com/georgeburry/tokyo-rental-prices/tree/master][similar project]] in 2018:
#+begin_src python :noweb-ref search-url :eval yes :results silent
# this is the URL generated after choosing specific search criteria on the website (e.g. location, house type, price range)
search_url = "http://suumo.jp/jj/chintai/ichiran/FR301FC001/?ar=030&bs=040&ta=13&sc=13101&sc=13102&sc=13103&sc=13104&sc=13105&sc=13113&cb=0.0&ct=9999999&et=9999999&cn=9999999&mb=0&mt=9999999&shkr1=03&shkr2=03&shkr3=03&shkr4=03&fw2="
#+end_src

*** Parsing
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 08:37]
:ID:       551be45d-5803-4e1b-ae3c-8afd7a4e172e
:END:

Lets do an initial parsing of the web content by counting the number of pages returned via a function. By inspecting the HTML elements we need to look inside the class =cassetteitem=. All entries related to the search are split into pages by looking for =pagination-parts= class instances.

#+begin_src python :noweb-ref scrape-functions :eval yes :results silent
def suumo_results_pages():
    """Return the number of pages generated by the search url"""
    requests = requests.get(search_url)
    c = r.content
    soup = BeautifulSoup(c,"html.parser")
    all = soup.find_all("div", class_="cassetteitem")
    page_nr = soup.find_all("ol", class_="pagination-parts")[-1].text
    page_nr = [int(s) for s in page_nr.split() if s.isdigit()]
    page_nr = page_nr[len(page_nr)-1]
    return page_nr

#+end_src

#+begin_src python :eval yes
print(suumo_results_pages(),"pages were found")
#+end_src

#+RESULTS:

As we can see, the original still works - albeit with more results than the original.

*** Collection of house elements
:PROPERTIES:
:CREATED:  [2023-05-28 Sun 12:59]
:ID:       63efe878-e4dd-4ce8-875e-112b46c34442
:END:

1. Create session object outside loop via =requests.Session()= for connection pooling.
2. Iterate pages by suffixing a pagination keyword (=&page=) on the url, adding page number to end of search URL each loop.
3. Use a more specific CSS selector for =cassetteitem=, =div.cassetteitem=.
4. Build this into the target collection of houses with a generator at =property_list=, saving on memory by yielding house elements one by one.
5. ~try~ and ~except~ for raising errors on pulling a particular page.

#+begin_src python :noweb-ref scrape-functions :results silent
def house_collector(start_page, end_page):
    """Build list of properties by looping through pages of search."""
    session = requests.Session()
    paginated_url = search_url + '&page='

    def generate_house_elements():
        for page in range(start_page, end_page):
            try:
                response = session.get(paginated_url + str(page))
                response.raise_for_status()  # Raise an exception if the request was not successful
                soup = BeautifulSoup(response.content,"html.parser")
                # "cassetteitem" is the class for each house
                yield from soup.select('div.cassetteitem')
                sleep(randint(1,3))
            except requests.exceptions.RequestException as e:
                print(f"Error occurred while fetching page {page}: {e}")

    property_list = list(generate_house_elements())
    return property_list

#+end_src

We can tell that a given page contains 30 results:
#+begin_src python :eval no
sum(1 for _ in house_collector(1, 2))
#+end_src

#+RESULTS:
: 30

Lets test for this to make sure we're getting the same kind of results for a given page:
#+begin_src python :noweb-ref tests :results silent
def test_house_collector():
    expected_houses = 30
    # houses = house_collector(1, 2) # TODO use randint() = x-1
    # Check if the houses are collected correctly
    assert sum(1 for _ in house_collector(1, 2)) == 30
    # for house, expected_house in zip(houses, expected_houses):
    #     assert house.text.strip() == expected_house
#+end_src

#+begin_src sh :session tests :eval yes
pytest tests/test_suumo.py::test_house_collector
#+end_src

#+RESULTS:
: /usr/bin/sh: 3: pytest: not found


*** Title details
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 13:40]
:ID:       f8b43fd2-5b08-4b4f-affe-ab5873da3515
:END:

The initial header of a given entry is contained in the =cassetteitem-detail= div, and contains the building name and some other information note found in the table used later on. For each house discovered, let's collect information on title, locality, and put the information into a dictionary:

#+begin_src python :noweb-ref scrape-functions :results silent
def extract_detail_text(html):
    """Extract header data from outside table"""
    house_data = []
    for item in html:
        d = {}
        d["Title"] = item.find("div",{"class","cassetteitem_content-title"}).text
        d["Locality"] = item.find("li",{"class","cassetteitem_detail-col1"}).text
        house_data.append(d)
    return house_data

#+end_src

As we can see, this gives us what we're looking for.
#+begin_src python :eval no
print(extract_detail_text(house_collector(1, 2))[0])
#+end_src

#+RESULTS:
: {'Title': 'アジールコート芝公園', 'Locality': '東京都港区芝２'}

We won't end up using this code in the extraction phase.

*** Table extraction
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 13:41]
:ID:       7ed8c278-8155-4a58-9c73-027683515ad1
:END:

Within each =cassetteitem= ~div~, there is a table containing the individual apartments associated with that particular building. These are stored in ~tr~ rows with the class =js-cassette_link=.

#+begin_src python
def bukken_by_row(html):
    house_data = []
    for cassetteitem in html:
        try:
            title = cassetteitem.find("div", class_="cassetteitem_content-title").text.strip()
            table = cassetteitem.find('table', class_='cassetteitem_other')
            rows = table.select('tbody > tr.js-cassette_link')
            # rows = cassetteitem.select('table.cassetteitem_other tbody > tr.js-cassette_link')
            house_data = [row.find('span.cassetteitem_menseki').text.strip() for row in rows]

            yield title, house_data
            # for row in rows:
            #     area = row.find('span', class_='cassetteitem_menseki').text.strip()

            #     house_data.append(area)
            # print(title)
        except AttributeError:
            # Handle missing elements or other exceptions
            pass
    # return house_data

sum(1 for _ in bukken_by_row(house_collector(1, 2)))
# bukken_by_row(house_collector(1, 2))

#+end_src

#+RESULTS:
: 0

'間取り' (madori) refers to the house plan, rendered in the =XLDK= format, where X is the number of rooms and D and K respectively refer to Dining room and Kitchen, and are optional. As is standard with Japanese listings, this is also often accompanied by an actual floor plan graphic.

TODO, use title function in place of explicit entry below.
#+begin_src python :noweb-ref scrape-functions :results silent
def extract_house_data(html):
    """Extract text from row data in table"""
    house_data = []
    for cassetteitem in html:
        table = cassetteitem.find('table',{'class','cassetteitem_other'})
        rows = table.find_all('tr', class_='js-cassette_link')
        for row in rows:
            columns = row.find_all('td')
            row_data = {
                'Title': extract_title(cassetteitem),
                'Locality': extract_locality(cassetteitem),
                'Floor': extract_floor(columns),
                'Rent': extract_rent(columns),
                'Admin Fee': extract_admin_fee(columns),
                'Deposit': extract_deposit(columns),
                'Key money': extract_key_money(columns),
                'Layout': extract_layout(columns),
                'Size': extract_size(columns),
                'ID': extract_id(columns),
                'Coordinates': extract_gps_location(row),
                'Link': extract_link(row),
            }
            house_data.append(row_data)
    return house_data

#+end_src

#+begin_src python :noweb-ref scrape-functions :results silent
def extract_title(cassetteitem):
    return cassetteitem.find('div', {'class', 'cassetteitem_content-title'}).text

def extract_locality(cassetteitem):
    return cassetteitem.find('li', {'class', 'cassetteitem_detail-col1'}).text

def extract_floor(cassetteitem):
    columns = cassetteitem.find_all('td')
    return columns[2].get_text().strip()

def extract_rent(cassetteitem):
    columns = cassetteitem.find_all('td')
    return columns[3].find('span', class_='cassetteitem_price--rent').text

def extract_admin_fee(cassetteitem):
    columns = cassetteitem.find_all('td')
    admin_fee = columns[3].find('span', class_='cassetteitem_price--administration')
    return admin_fee.get_text().strip() if admin_fee else ''

def extract_deposit(cassetteitem):
    columns = cassetteitem.find_all('td')
    deposit = columns[4].find('span', class_='cassetteitem_price--deposit')
    return deposit.get_text().strip() if deposit else ''

def extract_key_money(cassetteitem):
    columns = cassetteitem.find_all('td')
    key_money = columns[4].find('span', class_='cassetteitem_price--gratuity')
    return key_money.get_text().strip() if key_money else ''

def extract_layout(cassetteitem):
    columns = cassetteitem.find_all('td')
    layout = columns[5].find('span', class_='cassetteitem_madori')
    return layout.get_text().strip() if layout else ''

def extract_size(cassetteitem):
    columns = cassetteitem.find_all('td')
    size = columns[5].find('span', class_='cassetteitem_menseki')
    return size.get_text().strip() if size else ''

def extract_link(cassetteitem):
    row = cassetteitem.find('tr', class_='js-cassette_link')
    link = row.find('a', class_='js-cassette_link_href')
    return "https://suumo.jp" + link['href'] if link else ''

#+end_src

Getting the first member of the generated list shows a desirable dictionary entry:
print(extract_table_text(house_collector(1, 2))[1])
#+begin_src python
#+end_src

#+RESULTS:
: {'Title': 'トルナーレ日本橋浜町', 'Locality': '東京都中央区日本橋浜町３', 'Floor': '36階', 'Rent': '19万円', 'Admin Fee': '10000円', 'Deposit': '19万円', 'Key money': '19万円', 'Layout': 'ワンルーム', 'Size': '44.01m2', 'Link': 'https://suumo.jp/chintai/jnc_000082906762/?bc=100325224283'}
*** Load dataframe function
:PROPERTIES:
:CREATED:  [2023-06-04 Sun 09:14]
:ID:       ed9d39e5-119a-42a2-a619-a7ae5ea63a32
:END:

Let's create a simple function to load the df into memory for the given results page range.
Use =try-except= block to catch exceptions during the data loading process.
# TODO Consider using types

#+begin_src python :noweb-ref scrape-functions :results silent
def load_data(start_page, end_page):
    """Load the data into a DataFrame for the given results page range."""
    try:
        extracted_data = extract_house_data(house_collector(start_page, end_page))
        df = pd.DataFrame(extracted_data, columns=['Title', 'Locality', 'Floor', 'Size', 'Layout', 'Rent', 'Link'])
        return df
    except Exception as e:
        print(f"Error occurred while loading data: {e}")
        return None

#+end_src

Lets take a look at the initial frame:
#+begin_src python :results yes
df = load_data(1, 2)
df.head()
#+end_src

#+RESULTS:
#+begin_example
              Title   Locality Floor     Size Layout    Rent  \
0  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    5階  65.72m2   3LDK    33万円
1  ザ・グランクラッセ日本橋イースト  東京都中央区新川２   12階  65.72m2   3LDK  33.7万円
2  ザ・グランクラッセ日本橋イースト  東京都中央区新川２   11階   71.7m2   3LDK  35.3万円
3  ザ・グランクラッセ日本橋イースト  東京都中央区新川２   12階   71.7m2   3LDK  35.4万円
4  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    7階  71.44m2   3LDK  35.4万円

                                                Link
0  https://suumo.jp/chintai/jnc_000079775721/?bc=...
1  https://suumo.jp/chintai/jnc_000082788184/?bc=...
2  https://suumo.jp/chintai/jnc_000080944199/?bc=...
3  https://suumo.jp/chintai/jnc_000082788185/?bc=...
4  https://suumo.jp/chintai/jnc_000082479900/?bc=...
#+end_example

As we can see, our frame is created correctly, however there are entries that are non-numeric which we actually want as number values in order to begin EDA:
#+begin_src python
df['Rent'].dtype
#+end_src

#+RESULTS:
: dtype('O')

Which is not supported by =Numpy=.

** TODO Research and identify additional rental data sources
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       7c6311eb-30e3-4144-9b35-fe323edcf08f
:END:
Research and identify additional sources of rental data in Tokyo to enrich the dataset.

* TODO Cleaning
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       8c93d6a6-282a-4890-974d-0c209b874cf2
:END:
** NEXT Apply data cleaning techniques
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       e79c734c-70ef-4230-9911-806019735e1c
:TRIGGER:  chain-find-next(NEXT,from-current,priority-up,effort-down)
:END:
Apply data cleaning techniques to address inconsistencies, missing values, and outliers in the rental data.

We need to reconfigure our data frame so that relevant columns contain numerical values. We also will be inserting a new column =Rooms= to represent how many liveable rooms there are without losing access to the XLDK layout convention:

Use input validation to ensure a valid Pandas DataFrame or Series and use a DataFrame Copy to ensure immutablity of original dataframe.

For speed I use pre-compiled regexes via =re.compile()= outside the function body. Finally we do a simple test of the OG df to see if it needs to be cleaned, and further tests of unwanted strings in the respective columns before applying the reconfigures to avoid multiplying values unnecessarily.
#+begin_src python :noweb-ref clean-functions :results silent
def clean_numeric_data(dataframe: pd.DataFrame) -> pd.DataFrame:
    """
    Clean the dataframe generated by scraping to address inconsistencies, missing values, and outliers.

    Args:
        dataframe (pd.DataFrame): The input DataFrame to be cleaned.

    Returns:
        pd.DataFrame: The cleaned DataFrame.
    """
    if not isinstance(dataframe, (pd.DataFrame, pd.Series)):
        raise ValueError("Input must be a Pandas DataFrame or Series.")

    df = dataframe.copy()

    # Pre-compile regular expressions
    decimal_value = re.compile(r'(\d+(?:\.\d+)?)')
    int_value = re.compile(r'\d+')

    # Check if respective column needs cleaning
    if not df.empty:
        if df['Floor'].str.contains("階").any():
            df['Floor'] = df['Floor'].apply(lambda x: re.findall(int_value, x)[0]
                                            if re.findall(int_value, x)
                                            else '')
            df['Rooms'] = df['Layout'].apply(lambda x: re.findall(int_value, x)[0]
                                        if re.findall(int_value, x)
                                        else '1' if 'ワンルーム' in x
                                        else '')
        if df['Size'].str.contains("m2").any():
            df['Size'] = df['Size'].apply(lambda x: re.findall(decimal_value, x)[0]
                                        if re.findall(decimal_value, x)
                                        else '')
        if df['Rent'].str.contains("円").any():
            # df['Rent'] = df['Rent'].apply(lambda x:
            #                             int(float(re.findall(decimal_value, x)[0]) * 1000)
            #                             if '万' in x and re.findall(decimal_value, x)
            #                             else '')
            df['Rent'] = df['Rent'].str.extract(decimal_value, expand=False)
            df['Rent'] = df['Rent'].astype(float).astype(int) * 10000
        return df

#+end_src

Now lets apply our data cleaning and take a look at the new frame:
#+begin_src python
df_cleaned = clean_numeric_data(load_data(1, 2))
df_cleaned.head()
#+end_src

#+RESULTS:
#+begin_example
              Title   Locality Floor   Size Layout    Rent  \
0  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     5  65.72   3LDK  330000
1  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12  65.72   3LDK  330000
2  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    11   71.7   3LDK  350000
3  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12   71.7   3LDK  350000
4  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     7  71.44   3LDK  350000

                                                Link Rooms
0  https://suumo.jp/chintai/jnc_000079775721/?bc=...     3
1  https://suumo.jp/chintai/jnc_000082788184/?bc=...     3
2  https://suumo.jp/chintai/jnc_000080944199/?bc=...     3
3  https://suumo.jp/chintai/jnc_000082788185/?bc=...     3
4  https://suumo.jp/chintai/jnc_000082479900/?bc=...     3
#+end_example

Our Rent column returns as the correct datatype:
#+begin_src python
df_cleaned['Rent']
#+end_src

#+RESULTS:
#+begin_example
0      330000
1      330000
2      350000
3      350000
4      350000
        ...
209    150000
210    150000
211    150000
212    150000
213    160000
Name: Rent, Length: 214, dtype: int64
#+end_example

#+begin_src python
df_cleaned.loc[1]
# df[df['Title'] == 'クリオ日本橋久松町']
# df.loc[1, 'Link']
#+end_src

#+RESULTS:
: Title                                        ザ・グランクラッセ日本橋イースト
: Locality                                            東京都中央区新川２
: Floor                                                      12
: Size                                                    65.72
: Layout                                                   3LDK
: Rent                                                   330000
: Link        https://suumo.jp/chintai/jnc_000082788184/?bc=...
: Rooms                                                       3
: Name: 1, dtype: object

* TODO Develop data collection pipeline
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       630ccbf5-6f99-40ae-9f6e-2ec5541f04c2
:END:
Develop a data collection pipeline or script to automate the gathering of rental data from various sources.

We'll to use the ~pandas.DataFrame.pipe~ to setup a simple data pipeline that runs from extraction via scraping, through our exploratory and cleaning dataframe transformations and ending in loading into an sqlite3 database; effectively giving us an ETL pipeline.

This demonstrated below:
df_cleaned = load_data(1, 2).pipe(clean_numeric_data)
df_cleaned.head()
#+begin_src python
#+end_src

#+RESULTS:
#+begin_example
              Title   Locality Floor   Size Layout    Rent  \
0  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     5  65.72   3LDK  330000
1  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12  65.72   3LDK  330000
2  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    11   71.7   3LDK  350000
3  ザ・グランクラッセ日本橋イースト  東京都中央区新川２    12   71.7   3LDK  350000
4  ザ・グランクラッセ日本橋イースト  東京都中央区新川２     7  71.44   3LDK  350000

                                                Link Rooms
0  https://suumo.jp/chintai/jnc_000079775721/?bc=...     3
1  https://suumo.jp/chintai/jnc_000082788184/?bc=...     3
2  https://suumo.jp/chintai/jnc_000080944199/?bc=...     3
3  https://suumo.jp/chintai/jnc_000082788185/?bc=...     3
4  https://suumo.jp/chintai/jnc_000082479900/?bc=...     3
#+end_example

Now lets have this as reproducible functions.

*** Piped database creation
:PROPERTIES:
:CREATED:  [2023-06-04 Sun 16:20]
:ID:       01b405a2-05c5-4280-8b78-fbab4d28e433
:END:

Here we set variable name with the string of a yearly table. Then we use that variable when invoking the =to_sql= method on the piped object, which returns a cleaned pandas DataFrame.

We set the =if_exists= arguments to ='replace'= so that the code doesn't fail if the table already exists in the database. We can also change =if_exists= to ='append'= and add exception handling in a more robust version of this program.

Create =suumo.db= and establish connection to resultant database:
#+begin_src python :noweb-ref database-functions :results silent
def create_database(db, table, start_page, end_page):
    engine = create_engine('sqlite:///%s' %db, echo=True)
    sqlite_table = table
    sqlite_connection = engine.connect()
    (load_data(start, end)
    .pipe(clean_numeric_data).to_sql(
        sqlite_table,
        sqlite_connection,
        if_exists='replace',
        index=False
    ))
    sqlite_connection.close()

#+end_src

Turn into test
create_database("suumo-test.db", "Suumo2023_test", 1, 2)
#+begin_src python :results silent :eval yes
#+end_src

#+end_example

* TODO Analysis
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       8ce6c8e1-1d6e-4321-a723-b3e1e4892cb3
:END:
** TODO Perform exploratory data analysis
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       32c93679-55fa-4e6a-9ce0-5e2125d0213d
:END:
Perform exploratory data analysis to gain insights into rental price distribution, property types, and geographical variations.

#+begin_src python
** TODO Implement statistical analysis techniques
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       00a104bd-41e3-4f87-ae4e-c6741fa4ef09
:END:
Implement statistical analysis techniques such as regression, clustering, or time series analysis to identify patterns and trends in the rental market.

* TODO Visualization
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       0bfc3db3-552e-458f-8127-5761d40b4eb2
:END:
*** TODO Create interactive visualizations
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       947558a4-7652-4a83-89e4-8e69b031f364
:END:
Create interactive visualizations using Dash or other libraries to present rental data in an intuitive and user-friendly manner.
*** TODO Conduct comparative analysis
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 17:02]
:ID:       0cd53336-9b42-49a2-873a-566cc58678fd
:END:
Conduct comparative analysis between different districts or neighborhoods within Tokyo to identify affordable rental options or investment opportunities.

* TODO Data Sharing
:PROPERTIES:
:CREATED:  [2023-05-23 Tue 16:28]
:ID:       0d22c9cc-a8e8-45fa-927d-7369eceae898
:END:

* Files
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 18:06]
:ID:       9007fc1c-9c66-434a-8cb3-5227d6b0d9c0
:END:

** suumo.py
:PROPERTIES:
:CREATED:  [2023-06-03 Sat 18:07]
:ID:       4396c626-15b4-4752-ad41-3ead8942475e
:END:

#+begin_src python :tangle src/suumo/suumo.py
# Tools for scraping SUUMO
<<requirements>>

<<search-url>>

<<scrape-functions>>

<<clean-functions>>

#+end_src
